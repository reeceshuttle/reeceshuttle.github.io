<html>
  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

    <link rel="shortcut icon" href="images/icon.ico" />
    <style type="text/css">
      body {
      	background-color: #f5f9ff;
      }

      /* Hide both math displays initially, will display based on JS detection */
       .mathjax-mobile, .mathml-non-mobile { display: none; }

       /* Show the MathML content by default on non-mobile devices */
       .show-mathml .mathml-non-mobile { display: block; }
       .show-mathjax .mathjax-mobile { display: block; }

      .content-margin-container {
      	display: flex;
      	width: 100%; /* Ensure the container is full width */
      	justify-content: left; /* Horizontally centers the children in the container */
      	align-items: center;  /* Vertically centers the children in the container */
      }
      .main-content-block {
      	width: 70%; /* Change this percentage as needed */
         max-width: 1100px; /* Optional: Maximum width */
      	background-color: #fff;
      	border-left: 1px solid #DDD;
      	border-right: 1px solid #DDD;
      	padding: 8px 8px 8px 8px;
      	font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      }
      .margin-left-block {
      		font-size: 14px;
      		width: 15%; /* Change this percentage as needed */
      		max-width: 130px; /* Optional: Maximum width */
      		position: relative;
      		margin-left: 10px;
      		text-align: left;
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		padding: 5px;
      }
      .margin-right-block {
      		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;#"Avenir";
      		font-size: 14px;
      		width: 25%; /* Change this percentage as needed */
      		max-width: 256px; /* Optional: Maximum width */
      		position: relative;
      		text-align: left;
      		padding: 10px;  /* Optional: Adds padding inside the caption */
      }

      img {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      .my-video {
      		max-width: 100%; /* Make sure it fits inside the container */
      		height: auto;
      		display: block;
      		margin: auto;
      }
      /* Hide both video displays initially, will display based on JS detection */
       .vid-mobile, .vid-non-mobile { display: none; }

       /* Show the video content by default on non-mobile devices */
       .show-vid-mobile .vid-mobile { display: block; }
       .show-vid-non-mobile .vid-non-mobile { display: block; }

      a:link,a:visited
      {
      	color: #0e7862; /*#1367a7;*/
      	text-decoration: none;
      }
      a:hover {
      	color: #24b597; /*#208799;*/
      }

      h1 {
      	font-size: 18px;
      	margin-top: 4px;
      	margin-bottom: 10px;
      }

      table.header {
         font-weight: 300;
         font-size: 17px;
         flex-grow: 1;
      	width: 70%;
         max-width: calc(100% - 290px); /* Adjust according to the width of .paper-code-tab */
      }
      table td, table td * {
          vertical-align: middle;
          position: relative;
      }
      table.paper-code-tab {
          flex-shrink: 0;
          margin-left: 8px;
          margin-top: 8px;
          padding: 0px 0px 0px 8px;
          width: 290px;
          height: 150px;
      }

      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
      	box-shadow:
      	        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
      	        5px 5px 0 0px #fff, /* The second layer */
      	        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
      	        10px 10px 0 0px #fff, /* The third layer */
      	        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
      	margin-top: 5px;
      	margin-left: 10px;
      	margin-right: 30px;
      	margin-bottom: 5px;
      }

      hr {
         height: 1px; /* Sets the height of the line to 1 pixel */
         border: none; /* Removes the default border */
         background-color: #DDD; /* Sets the line color to black */
       }

      div.hypothesis {
      	width: 80%;
      	background-color: #EEE;
      	border: 1px solid black;
      	border-radius: 10px;
      	-moz-border-radius: 10px;
      	-webkit-border-radius: 10px;
      	font-family: Courier;
      	font-size: 18px;
      	text-align: center;
      	margin: auto;
      	padding: 16px 16px 16px 16px;
      }

      div.citation {
         font-size: 0.8em;
         background-color:#fff;
         padding: 10px;
      	height: 200px;
       }

      .fade-in-inline {
      	position: absolute;
      	text-align: center;
      	margin: auto;
      	-webkit-mask-image: linear-gradient(to right,
      																		transparent 0%,
      																		transparent 40%,
      																		black 50%,
      																		black 90%,
      																		transparent 100%);
      	mask-image: linear-gradient(to right,
      															transparent 0%,
      															transparent 40%,
      															black 50%,
      															black 90%,
      															transparent 100%);
      	-webkit-mask-size: 8000% 100%;
      	mask-size: 8000% 100%;
      	animation-name: sweepMask;
      	animation-duration: 4s;
      	animation-iteration-count: infinite;
      	animation-timing-function: linear;
      	animation-delay: -1s;
      }

      .fade-in2-inline {
      		animation-delay: 1s;
      }

      .inline-div {
      		position: relative;
          display: inline-block; /* Makes both the div and paragraph inline-block elements */
          vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
          width: 50px; /* Optional: Adds space between the div and the paragraph */
      }
    </style>

    <title>Deep Learning Project</title>
    <meta property="og:title" content="Deep Learning Project" />
    <meta charset="UTF-8" />
  </head>

  <body>
    <div class="content-margin-container">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <table class="header" align="left">
          <tr>
            <td colspan="4">
              <span
                style="
                  font-size: 30px;
                  font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */
                "
                >Exploring the Causes & Effects of Quantization-induced
                Degradation in LLMs</span
              >
            </td>
          </tr>
          <tr>
            <td align="left">
              <span style="font-size: 17px"
                ><a href="https://reeceshuttle.me">Reece Shuttleworth</a></span
              >
            </td>
          </tr>

          <tr>
            <td colspan="4" align="left">
              <span style="font-size: 18px">MIT</span>
            </td>
          </tr>
        </table>
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block">
        <!-- table of contents here -->
        <div style="position: fixed; max-width: inherit; top: max(20%, 120px)">
          <b style="font-size: 16px">Contents</b><br /><br />
          <a href="#intro">Introduction</a><br /><br />
          <a href="#background">Background</a><br /><br />
          <a href="#methods">Methods</a><br /><br />
          <a href="#analysis">Analysis</a><br /><br />
          <a href="#discussion">Discussion</a><br /><br />
        </div>
      </div>

      <div class="main-content-block">
        <!--You can embed an image like this:-->
        <img src="./images/QiD_loss.png" width="768px" />
      </div>
      <div class="margin-right-block">
        <a id="fig_1"></a>Figure 1: Reproducing Quantization-induced Degradation
        (QiD). Both AWQ and naive zero-point RTN are used to quantize models and
        loss is measured on the C4 dataset. We see that as you train for longer,
        quantiation error increases. This confirms the findings of [<a
          href="#ref_1"
          >1</a
        >].
      </div>
    </div>

    <div class="content-margin-container" id="intro">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Introduction</h1>

        Current LLMs are pre-trained far into the overtrained regime: they are
        trained on many multiples of tokens greater than what is considered to
        be compute optimal (around 20 tokens per model parameter [<a
          href="#ref_8"
          >8</a
        >]). For example, LLaMA-3 8B [<a href="#ref_7">7</a>] was trained on
        more than 15 Trillion tokens, which approaches 2000 tokens per model
        parameter. Even though it is not compute optimal, this extended
        pre-training stage has been shown to improve performance and reduce
        inference costs given a particular level of performance [<a
          href="#ref_6"
          >6</a
        >].

        <br />
        <br />

        Another important method to reduce inference costs is quantization,
        which reduces the precision that weights (and sometimes activations) are
        stored in. This reduction in precision leads to memory and compute
        savings, because you can represent the model in fewer bits and can more
        easily move weights into GPU memory and do computations when the values
        have fewer bits.

        <br />
        <br />

        Generally, after training models are quantized before being served to
        the public. However, recent work has observed an interesting phenomenon
        in which models that are pre-trained for longer have higher loss after
        post-training quantization (PTQ) than models that were trained on fewer
        tokens[<a href="#ref_1">1</a>,<a href="#ref_2">2</a>]. This leads to a
        clear conflict, because pre-training for longer leads to better
        performance and enables smaller models to be used given a fixed level of
        performance.

        <br /><br />

        This observation suggests that there will be a clear limit to the amount
        we can quantize a model, which may place limits on the efficiency gains
        we can get from techniques. While the current findings to not impact
        existing methods because they are occuring at very low bits, like 3 bit,
        as models get pre-trained for longer this observation has the potential
        to also effect current methods.
        <br /><br />

        It is not clear why this phenomenon is occuring and what the
        consequences of it will be. In this work, we aim to investigate the
        causes and effects of this phenomenon. Previous work has shown that LLMs
        with better performance have large activations with a higher frequency
        [<a href="#ref_4">4</a>]. These large activations are difficult to
        quantize without errors and are important for model performance [<a
          href="#ref_4"
          >4</a
        >]. Could these be the culprit behind this degradation in performance?
        Furthermore, this phenomenon has only been observed in loss. What are
        the impacts of this on downstream task performance? Lastly, can we find
        the specific part of the model that is breaking and causing this
        phenomenon?

        <br /><br />

        Our results can be reproduced with our codebase, which can be found
        <a
          href="https://github.com/reeceshuttle/deep-learning-proj"
          target="_blank"
          ><u>here.</u></a
        >
        <br />
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="background">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Background</h1>
        <h5>Quantization</h5>

        Quantization takes values of a certain precision and converts them to a
        lower precision. This is a lossy process. For example, converting from
        FP32 to FP16 may lead cause the number to lose some of its least
        significant values. Making this conversion is good performance, because
        with this change it is easier to store and move weights and more
        efficient to do MACs.

        <br />

        The goal of model quantization is to reduce the size of the model while
        preserving performance as much as possible. While there are many ways to
        do quantization, this work focuses on two: zero-point round-to-nearest
        (RTN) and activation-aware quantization (AWQ). Zero-point RTN works by
        rounding every value to its nearest corresponding value in a new range,
        which is dictated by the maximum and minimum number in the set of
        numbers that are going to be quantized, as shown in the graphic below.
        We call this method 'naive' in this article going forward because it is
        not informed by the activations: it only acts on the weights.

        <br />

        In contrast, AWQ works by implementing the same method as zero-point
        RTN, but with a twist: before quantization, weights are scaled up and
        activations are scaled down in order to reduce the quantization error
        associated with quantizing large activations. These scaling factors are
        commonly found via search and using a small calibration set in order to
        identify channels that commonly have large activations. AWQ has been
        shown to reduce quantization loss for LLMs.
        <img src="./images/quant_example.png" width="256px" />

        <h5>Quantization-induced Degradation</h5>

        Recent work has found that models that are pre-trained for longer have
        higher loss after post-training quantization (PTQ) than models that were
        trained on fewer tokens[<a href="#ref_1">1</a>,<a href="#ref_2">2</a>].
        Specifically, they find that when using 3-5 bits of weight-only
        quantization, loss goes up more on models pre-trained for longer. This
        phenomenon is called Quantization-induced Degradation. Eventually,
        models trained for a long time actually have higher loss post
        quantization than lose trained for a shorter period of time. This can be
        observed in our <a href="#fig_1">Figure 1</a>, where we reproduced the
        finding.

        <h5>Large Activations in LLMs</h5>

        Previous work has found that LLMs, particularly those trained to low
        perplexity, have large activations. These large activations generally
        occur in specific channels[<a href="#ref_4">4</a>] for tokens and
        certain tokens like space or new line characters[<a href="#ref_15">15</a
        >]. These are difficult to quantize becuase they increase the
        quantization range (as described above) and increase the error. To
        combat this, techniques like Mixed-Precision[<a href="#ref_4">4</a>] and
        AWQ[<a href="ref_5">5</a>] have been developed to handle these large
        activations and reduce the quantization error. talk about llm int8 and
        massive activations findings.
        <img src="./images/QiD_task_perf.png" width="1024px" />
      </div>
      <div class="margin-right-block">
        Figure 2: In contrast to Figure q, when we measure task performance of
        the quantized models, we see that 4 and 5 bit perform almost
        equivalently to full precision. This is across 3 datasets and two
        quantization methods.
      </div>
    </div>

    <div class="content-margin-container" id="methods">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Methods</h1>
        For all of our studies, we use the OLMo-1B[<a href="#ref_9">9</a>] class
        of models. What makes these powerful for research is that it was trained
        for 3 trillion tokens and, importantly, checkpoints were frequently
        taken during training. They released all checkpoints to the public. This
        gives us a sequence of models across pre-training that we can then use
        to study QiD.
        <h5>Testing the Effect of QiD on Task Performance</h5>
        Previously, QiD has only been observed on language modelling loss. We
        are interested in seeing how task performance is impacted by this
        effect. To measure this, we test the quantized OlMo models across
        training steps on several common tasks for benchmarking LLMs. We select
        PIQA[<a href="#ref_11">11</a>], Winogrande[<a href="#ref_12">12</a>],
        and LAMBADA[<a href="#ref_13">13</a>], because they are found to be
        strong predictors of improved language modelling[<a href="#ref_3">3</a
        >]. We use the lm-evaluation-harness[<a href="#ref_10">10</a>] to
        evaluate the models and measure zero-shot accuracy for all tasks. We
        describe our results below and report our results in Figure 2.
        <h5>Testing the Impact of Activations on QiD</h5>
        We look for a link between the activations and QiD in two ways.
        <br />
        First, we study the activations of our OLMo models across training steps
        by tracking several statistics associated with activation size. The
        activations we study are created from an example of sequence length 512
        and are recorded after every single weight matrix in the model. We
        record and report 5 statistics across training steps: the average across
        layers of the absolute mean of the activations, the average across
        layers of the absolute maximum in the activations, and the raw number of
        activations above 1, 10, and 100 respectively. If the size of
        activations appear to go up significantly when QiD onsets, then we would
        have evidence of the effect of large activations on the observed
        phenomenon of QiD. We report our findings below and in Figure 3.

        <br />
        Second, we implicitly study the impact of the activations on
        quantization error by measuring the difference in quantization error
        between naive zero-point RTN and AWQ. Since AWQ is activation aware
        while our naive method is not, if activations play a big role in this
        phenomenon then we should expect that AWQ will reduce the quantization
        error of the models. Is it the case that AWQ doesn't exhibit QiD but
        naively doing zero-point RTN does? To test this, we quantize the OLMo-1B
        models across training steps with both methods and compare the loss and
        task performance that both methods achieve. If AWQ gets much less loss
        and much better task performance than our naive method, then we would
        have evidence of the importance of handling activations correctly in
        order to reduce the phenomenon of QiD. These results are found in Figure
        1 & 2.
        <br />

        <h5>Testing what Model Component is Causing QiD</h5>
        We are interested in diagnosing exactly why QiD is occuring. It may not
        be the case that the entire model is degrading equally and certain
        components or aspects of the model may be the cause of our observations.
        <br />
        To test if there are certain modules in the model that have an increase
        in loss in comparison to full precision we measure the norm of the
        difference in the activations outputted by every weight matrix between
        the full precision and each quantized model. This can be thought of as
        measuring the error in the activations incurred by the model at a
        certain step of the forward pass because of quantization. If we can find
        that a certain module has constant "activation quantization loss" while
        examining it across training steps, it likely isn't responsible for QiD.
        In contrast, if a module has increasing "activation quantization loss",
        it is likely contributing to QiD. We report this experiment in Figure 4.

        <br />
        <img src="./images/16bit_statistics.png" width="1024px" />
      </div>
      <div class="margin-right-block">
        Figure 3: We measure statistics of the activations across OLMo-1B
        training steps. We find that the the size of activations actually appear
        to <i>decrease</i> across training steps, suggesting that they are not
        responsible of QiD.
      </div>
    </div>

    <div class="content-margin-container" id="analysis">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Results & Analysis</h1>
        Here, we discuss the results of our experiments as described in the
        previous section.
        <h5>Testing the Effect of QiD on Task Performance</h5>
        Examining Figure 2, we see that 3 bit quantization has a strong
        degradation in task performance for all three datasets, irrespective of
        the quantization method. However, we do notice that for 4 and 5 bit
        quantization, both naive and AWQ are able to match or nearly match the
        performance of full precision across OLMo's training steps, even when
        loss is not similar due to QiD. This is an interesting finding and shows
        that QiD will not be universal for all things that the model does, and
        it suggests that more research is needed into what QiD is doing to the
        model and what it impacts.
        <br />
        This also suggests that language modelling loss may not be a perfect
        measurement for diagnosing practices that are good and bad for LLMs:
        what we care about is task performance, and loss here may be leading us
        astray.
        <h5>Testing the Impact of Activations on QiD</h5>
        When examining the statistics that we measured from the activations
        across training steps, we actually observe the reverse to what we
        expected: the size of activations appears to be going down across
        training steps, not up. This holds especcially well for the average
        absolute mean of the activations and the raw number of activations above
        1. Activations may not be the culprit of QiD.

        <br />
        <br />
        When examining the difference between naive quantization and AWQ, we
        find something similar to above: naive quantization and AWQ have very
        little difference between both their loss(Figure 1) and task
        performance(Figure 2). This further suggests that activations are
        unlikely to be causing QiD since naive quantization performs as well as
        AWQ.

        <br /><br />
        Because of these two, we have strong reasons to believe that activations
        are unlikely to be the cause of QiD.
        <h5>Testing what Model Component is Causing QiD</h5>

        Studying the plot for Figure 4, we look for patterns that are different
        across training steps, since as training occurs QiD gets worse. Across
        training steps and quantization levels, the norm of the activation error
        appears to be pretty similar for the attention output, feed forward
        projection, and feed forward output layers, regardless of the
        quantization technique used. However, We can notice that across training
        steps, the norm of the activation error of the attention projection
        module clearly grows across training steps. This finding is independent
        of quantization technique and the number of bit used for quantization.
        This is a <b>very</b> interesting finding. This suggests that
        <i>the attention projection module is responsible for QiD</i> while the
        other three modules appear to not have a notable change in activation
        error across training steps.
        <br />
        This finding suggests several remedies for future work. Could keeping
        the attention projection layer in higher precision, lets say 8bit, while
        keeping the other layers in low precision help allieviate QiD?
        <img src="./images/error_norm.png" width="1024px" />
      </div>
      <div class="margin-right-block">
        Figure 4: We measure the norm of the difference between the full
        precision and quantized activations for each layer number and type. We
        consider this the activation quantization error. We see that across
        training steps, the loss of the attention output, feed forward
        projection, and feed forward output layers have similar error. However,
        the attention projection layer (red line) has error that grows across
        training steps. This finding holds across different quantization levels.
        This suggests that the attention projection layer is reponsible for QiD.
      </div>
    </div>

    <div class="content-margin-container" id="discussion">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <h1>Discussion</h1>
        In this work, we investigated the impact of QiD on task performance, the
        impact of activations on QiD, and which model component is responsible
        for QiD. We found that although loss does degrade across training steps
        when you quantize the model, task performance stays similar to that of
        full precision. We found that large activations are not responsible for
        QiD. Lastly we found that the attention projection layer seems to be the
        culprit of QiD.

        <br />

        If large activations are not responsible for QiD, what is? Completely
        speculatively, it may be the case that as the model is trained for
        longer and gets more and more into a superposition [<a href="#ref_14"
          >14</a
        >], quantization somehow deletes information in a that harms the model
        more if it has been trained for longer.

        <br />
        This work has several limitations. Most notably, we only studied one
        model. Futher, this model is not very large, meaning certain phenomena
        may occur with it and not larger, more SOTA models. Other limitations
        include not isolating the causes of the things we have observed:
        currently they are only observations. We leave to future work
        replicating our study on OLMo-7B and examining if keeping the attention
        projection module in full precision eliminates QiD.
      </div>
      <div class="margin-right-block"></div>
    </div>

    <div class="content-margin-container" id="citations">
      <div class="margin-left-block"></div>
      <div class="main-content-block">
        <div class="citation" id="references" style="height: auto">
          <br />
          <span style="font-size: 16px">References:</span><br /><br />
          <a id="ref_1"></a>[1]
          <a href="https://arxiv.org/pdf/2411.04330"
            >Scaling Laws For Precision</a
          >, Kumar et al. 2024<br /><br />

          <a id="ref_2"></a>[2]
          <a href="https://arxiv.org/pdf/2411.17691"
            >Low-Bit Quantization Favors Undertrained LLMs</a
          >, Ouyang et al. 2024<br /><br />

          <a id="ref_3"></a>[3]
          <a
            href="https://www.databricks.com/blog/calibrating-mosaic-evaluation-gauntlet"
            >Calibrating the Mosaic Evaluation Gauntlet</a
          >, Tessa Barton, 2024<br /><br />

          <a id="ref_4"></a>[4]
          <a href="https://arxiv.org/pdf/2208.07339">LLM.int8()</a>, Dettmers et
          al. 2022<br /><br />

          <a id="ref_5"></a>[5]
          <a href="https://arxiv.org/pdf/2306.00978">AWQ</a>, Lin et al. 2023<br /><br />

          <a id="ref_6"></a>[6]
          <a href="https://arxiv.org/pdf/2302.13971">LLaMA</a>, Touvron et al.
          2023<br /><br />

          <a id="ref_7"></a>[7]
          <a href="https://arxiv.org/pdf/2407.21783">LLaMA3</a>, Grattafiori et
          al. 2024<br /><br />

          <a id="ref_8"></a>[8]
          <a href="https://arxiv.org/pdf/2203.15556">Chinchilla</a>, Hoffmann et
          al. 2022<br /><br />

          <a id="ref_9"></a>[9]
          <a href="https://arxiv.org/pdf/2402.00838">OLMo</a>, Groeneveld et al.
          2024<br /><br />

          <a id="ref_10"></a>[10]
          <a href="https://github.com/EleutherAI/lm-evaluation-harness"
            >lm-evaluation-harness</a
          >, EleutherAI 2024<br /><br />

          <a id="ref_11"></a>[11]
          <a href="https://arxiv.org/abs/1911.11641">PIQA</a>, Bist et al.
          2019<br /><br />

          <a id="ref_12"></a>[12]
          <a href="https://arxiv.org/abs/1907.10641">Winogrande</a>, Sakaguchi
          et al. 2019<br /><br />

          <a id="ref_13"></a>[13]
          <a href="https://arxiv.org/abs/1606.06031">LAMBADA</a>, Paperno et al.
          2016<br /><br />
          <a id="ref_14"></a>[14]
          <a href="https://transformer-circuits.pub/2022/toy_model/index.html"
            >Toy Models of Superposition</a
          >, Elhage et al. 2016<br /><br />
          <a id="ref_15"></a>[15]
          <a href="https://arxiv.org/abs/2402.17762"
            >Massive Activations in Large Language Models</a
          >, Sun et al. 2024<br /><br />
        </div>
      </div>
      <div class="margin-right-block">
        <!-- margin notes for reference block here -->
      </div>
    </div>
  </body>
</html>
