<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Reece Shuttleworth</title>
    <link rel="icon" href="assets/signature.png" />
    <link rel="stylesheet" href="css/styles.css" />
    <!-- Add your stylesheets or other head elements here -->
    <link
      href="https://fonts.googleapis.com/css?family=DM Sans"
      rel="stylesheet"
    />
  </head>
  <body>
    <!-- <div class="landing-page">
      <h1>Welcome to Reece Shuttleworth's website.</h1>
      <h2 class="fade-in">(click anywhere to continue)</h2>
    </div> -->

    <div class="container-header">
      <header>
        <h1>Reece Shuttleworth</h1>
        <nav>
          <ul>
            <li><a href="#home">Home</a></li>
            <li><a href="#bio">My Bio</a></li>
            <li><a href="#projects">My Work</a></li>
            <li><a href="assets/resume.pdf" target="_blank">Resume</a></li>
          </ul>
        </nav>
      </header>
    </div>
    <div class="container-content">
      <section id="home">
        <div class="two-pics-and-description">
          <img src="assets/pigeon.jpeg" alt="pic w pigeons" />
          <div class="description-with-buttons">
            <h2>My Table of Contents</h2>
            <hr />
            <div class="table-of-contents">
              <ol>
                <li><a href="#about">Bio</a></li>
                <li><a href="#projects">Projects & Publications</a></li>
                <li><a href="#blog">Blog</a></li>
              </ol>
            </div>
            <hr />
            <div class="row-of-buttons">
              <ul>
                <li>
                  <a href="https://github.com/reeceshuttle" target="_blank"
                    ><button>
                      <img
                        src="assets/github-mark.png"
                        alt="button to my github"
                      /></button
                  ></a>
                </li>
                <li>
                  <a
                    href="https://scholar.google.com/citations?user=J1d4PXYAAAAJ&hl=en"
                    target="_blank"
                    ><button>
                      <img
                        src="assets/google-scholar-circular.png"
                        alt="button to my google scholar"
                      /></button
                  ></a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/reece-shuttleworth/"
                    target="_blank"
                    ><button>
                      <img
                        src="assets/linkedin-circular.png"
                        alt="button to my linkedin"
                      /></button
                  ></a>
                </li>
              </ul>
            </div>
          </div>
          <img src="assets/suit-selfie2.jpeg" alt="pic w suit on" />
        </div>
      </section>
      <hr />
      <section id="bio">
        <h2>Bio</h2>
        <p>
          Hello, I'm Reece! I want to create safe AI that aims to solve our most
          important and pressing problems. Feel free to contact me at
          <a href="mailto:rshuttle@mit.edu">rshuttle@mit.edu</a>.
        </p>
        <p>
          Currently, I am an MEng student at MIT, working to understand how and
          why transformers and other deep neural nets work in hopes to make
          better models. I am primarily doing this work as a part of Jacob
          Andreas's lab. I also did my undergrad at MIT in 2.5 years, studying
          computer science and cognitive science.
          <!-- mention that is major is good for ML/AI? Also, mention poggios name?-->
        </p>
        <p>
          This past summer, I interned in the Bay Area at
          <a href="https://numenta.com" target="_blank">Numenta</a>, and worked
          on parameter efficient fine-tuning methods for sparse transformer
          models in order to reduce the hardware requirements associated with
          fine-tuning. Previously, I worked in CSAIL in the Learning and
          Intelligent Systems group, where I worked on
          <a
            href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J1d4PXYAAAAJ&citation_for_view=J1d4PXYAAAAJ:d1gkVwhDpl0C"
            target="_blank"
            >solving planning problems with AI.</a
          >
          Separately, I have also done work
          <a
            href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=J1d4PXYAAAAJ&citation_for_view=J1d4PXYAAAAJ:u5HHmVD_uO8C"
            target="_blank"
            >using Large Language Models</a
          >
          to solve math and science problems.
        </p>
        <p>
          I also run for
          <a
            href="https://mitathletics.com/sports/mens-cross-country/roster/reece-shuttleworth/11642"
            target="_blank"
            >MIT</a
          >.
        </p>
      </section>
      <hr />

      <section id="projects">
        <h1>Projects & Publications</h1>
        <h2>2024</h2>
        <h3>Check back later for updates üòÅ</h3>
        <h2>2023</h2>
        <h3>Characterizing Sparsity in Transformers</h3>
        <p>
          <a href="assets/9.58-Final-Project-Report.pdf" target="_blank"
            >[paper]</a
          >
          <a href="https://github.com/reeceshuttle/958" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Theoretical and biological reasons suggest that sparsity may be
          important for deep neural network to perform well. Here, we examine
          the attention blocks of large transformer models to identify sparse
          features in their weights and/or activations. One interesting finding
          is that the weight matricies used in attention have
          <i>very low stable rank</i>, especially the matrix product
          \(W_qW_k^T\).
        </p>
        <h3>Bias in BERT models</h3>
        <p>
          <a href="assets/6.3950-Final-Project-Report.pdf" target="_blank"
            >[paper]</a
          >
          <a href="https://github.com/reeceshuttle/63950" target="_blank"
            >[code]</a
          >

          <a
            href="https://wandb.ai/finetuning-bert/finetuning-bert?workspace=user-reece-shuttle"
            target="_blank"
            >[WandB runs]</a
          >
        </p>
        <p>
          Historical training data used for training machine learning models can
          be biased, and machine learning models are susceptible to inheriting
          this bias. In this work, we use the masked token prediction
          capabilities of BERT models to show that they contain gender and
          racial bias. We create a dataset and use a novel loss function in
          order to reduce bias via finetuning. Preliminary analysis shows that
          this finetuning is successful at reducing bias, but needs to be
          examined further.
        </p>

        <h3>Gabor filter-constrained CNNs</h3>
        <p>
          <a href="assets/960-final-project.pdf" target="_blank">[paper]</a>
          <a
            href="https://github.com/samacqua/gabor-constrained-nns"
            target="_blank"
            >[code]</a
          >
        </p>

        <p>
          Humans have more general and robust object recognition capabilities in
          comparison to machines. In this work, we investigated whether
          constraining convolutional neural networks to be more human-like, via
          the use of Gabor filters, improves their performance and their
          robustness against adversarial attacks.
        </p>
        <h3>MIT Pokerbots</h3>
        <p>
          <a href="https://github.com/reeceshuttle/poker-bot" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          I competed in the 2023 MIT Pokerbots competition and placed in the top
          10%, resulting in a cash prize. The variant played in this competition
          was River of Blood Hold'em.
        </p>
        <h3>PyTorch, but in NumPy</h3>
        <p>
          <a href="https://github.com/reeceshuttle/numpytorch" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Implemented basic PyTorch functionality from scratch using only NumPy
          arrays. Neural networks converge and perform well on non-trivial
          problems.
        </p>
        <h2>2022</h2>
        <h3>PDDL Planning with Pretrained Large Language Models</h3>
        <p>
          <a href="assets/pddl_planning.pdf" target="_blank">[paper]</a>
          <a
            href="https://github.com/Learning-and-Intelligent-Systems/llm4pddl"
            target="_blank"
            >[code]</a
          >
        </p>
        <p><u>Published in the NeurIPS FMDM Workshop.</u></p>
        <p>
          <u>Paper Abstract</u>: We study few-shot prompting of pretrained large
          language models (LLMs) towards solving PDDL planning problems. We are
          interested in two questions: (1) To what extent can LLMs solve PDDL
          planning problems on their own? (2) How and to what extent can LLMs be
          used to guide AI planners? Recent work by Valmeekam et al. (2022)
          presents negative evidence for (1) in the classic blocks world domain.
          We confirm this finding, but expand the inquiry to 18 domains and find
          more mixed results with a few clear successes. For (2), we propose a
          simple mechanism for using good-but-imperfect LLM outputs to aid a
          heuristic-search planner. We also find that the LLM performance is due
          not only to syntactic pattern matching, but also to its commonsense
          understanding of English terms that appear in the PDDL.
        </p>
        <h3>Using Large Language Models to Solve College Math</h3>
        <p>
          <a href="assets/llms-for-math.pdf" target="_blank">[paper]</a>
          <a href="https://github.com/idrori/mathQ" target="_blank">[code]</a>
        </p>
        <p><u>Published in PNAS.</u></p>
        <p>
          <u>Paper Abstract</u>: We demonstrate that a neural network pretrained
          on text and fine-tuned on code solves mathematics course problems,
          explains solutions, and generates questions at a human level. We
          automatically synthesize programs using few-shot learning and OpenAI's
          Codex transformer and execute them to solve course problems at 81%
          automatic accuracy. We curate a dataset of questions from
          Massachusetts Institute of Technology (MIT)'s largest mathematics
          courses (Single Variable and Multivariable Calculus, Differential
          Equations, Introduction to Probability and Statistics, Linear Algebra,
          and Mathematics for Computer Science) and Columbia University's
          Computational Linear Algebra. We solve questions from a MATH dataset
          (on Prealgebra, Algebra, Counting and Probability, Intermediate
          Algebra, Number Theory, and Precalculus), the latest benchmark of
          advanced mathematics problems designed to assess mathematical
          reasoning. We randomly sample questions and generate solutions with
          multiple modalities, including numbers, equations, and plots. The
          latest GPT-3 language model pretrained on text automatically solves
          only 18.8% of these university questions using zero-shot learning and
          30.8% using few-shot learning and the most recent chain of thought
          prompting. In contrast, program synthesis with few-shot learning using
          Codex fine-tuned on code generates programs that automatically solve
          81% of these questions. Our approach improves the previous
          state-of-the-art automatic solution accuracy on the benchmark topics
          from 8.8 to 81.1%. We perform a survey to evaluate the quality and
          difficulty of generated questions. This work automatically solves
          university-level mathematics course questions at a human level and
          explains and generates university-level mathematics course questions
          at scale, a milestone for higher education.
        </p>
      </section>
      <hr />

      <section id="blog">
        <h2>Blog</h2>
        <p>Under Construction.</p>
      </section>
      <hr />
    </div>

    <footer>
      <p>Updated 02/01/2024.</p>
    </footer>
    <!-- Add your scripts or other body elements here -->
    <script src="js/index.js"></script>
    <script
      type="text/javascript"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    ></script>
  </body>
</html>
