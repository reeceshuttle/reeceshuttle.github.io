<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Reece Shuttleworth</title>
    <link rel="icon" href="assets/signature.png" />
    <link rel="stylesheet" href="css/styles.css" />
    <!-- Add your stylesheets or other head elements here -->
    <link
      href="https://fonts.googleapis.com/css?family=DM Sans"
      rel="stylesheet"
    />
  </head>
  <body>
    <!-- <div class="landing-page">
      <h1>Welcome to Reece Shuttleworth's website.</h1>
      <h2 class="fade-in">(click anywhere to continue)</h2>
    </div> -->

    <div class="container-header">
      <header>
        <h1>Reece Shuttleworth</h1>
        <nav>
          <ul>
            <li><a href="#home">Home</a></li>
            <li><a href="#publications">Publications</a></li>
            <li><a href="#projects">Projects</a></li>
            <li><a href="assets/resume.pdf" target="_blank">Resume</a></li>
          </ul>
        </nav>
      </header>
    </div>
    <div class="container-content">
      <section id="home">
        <div class="two-pics-and-description">
          <img src="assets/grad_pic.png" alt="grad pic" />
          <div class="description-with-buttons">
            <h3>Hello, I'm Reece.</h3>
            <hr />
            <p>
              I recently graduated from MIT (BS '24, MEng '25), where I studied
              CS and CogSci. Previously, I have done ML research/engineering
              internships at Google DeepMind, Numenta, and Cleanlab. I was also
              a YC summer fellow in 2025.
            </p>
            <hr />
            <p></p>
            <hr />

            <div class="row-of-buttons">
              <ul>
                <li>
                  <a href="https://github.com/reeceshuttle" target="_blank"
                    ><button>
                      <img
                        src="assets/github-black-logo.png"
                        alt="button to my github"
                      /></button
                  ></a>
                </li>
                <li>
                  <a
                    href="https://scholar.google.com/citations?user=J1d4PXYAAAAJ&hl=en"
                    target="_blank"
                    ><button>
                      <img
                        src="assets/google-scholar-circular.png"
                        alt="button to my google scholar"
                      /></button
                  ></a>
                </li>
                <li>
                  <a
                    href="https://www.linkedin.com/in/reece-shuttleworth/"
                    target="_blank"
                    ><button>
                      <img
                        src="assets/linkedin-circular.png"
                        alt="button to my linkedin"
                      /></button
                  ></a>
                </li>
              </ul>
            </div>
          </div>
          <img src="assets/nyc.png" alt="pic from nyc" />
        </div>
      </section>
      <hr />

      <section id="publications">
        <h1>Publications</h1>
        <h3>LoRA vs Full Finetuning: An Illusion of Equivalence</h3>
        <p>
          <a href="https://arxiv.org/pdf/2410.21228" target="_blank">[paper]</a>
          <a
            href="https://github.com/reeceshuttle/intruder-dimensions"
            target="_blank"
            >[code]</a
          >
        </p>
        <p><u>Accepted into NeurIPS '25.</u></p>

        <h3>PDDL Planning with Pretrained Large Language Models</h3>
        <p>
          <a href="assets/pddl_planning.pdf" target="_blank">[paper]</a>
          <!-- use this link: https://openreview.net/pdf?id=1QMMUB4zfl -->
          <a
            href="https://github.com/Learning-and-Intelligent-Systems/llm4pddl"
            target="_blank"
            >[code]</a
          >
        </p>
        <p><u>Published in the NeurIPS FMDM Workshop.</u></p>
        <!-- <p>
          <u>Paper Abstract</u>: We study few-shot prompting of pretrained large
          language models (LLMs) towards solving PDDL planning problems. We are
          interested in two questions: (1) To what extent can LLMs solve PDDL
          planning problems on their own? (2) How and to what extent can LLMs be
          used to guide AI planners? Recent work by Valmeekam et al. (2022)
          presents negative evidence for (1) in the classic blocks world domain.
          We confirm this finding, but expand the inquiry to 18 domains and find
          more mixed results with a few clear successes. For (2), we propose a
          simple mechanism for using good-but-imperfect LLM outputs to aid a
          heuristic-search planner. We also find that the LLM performance is due
          not only to syntactic pattern matching, but also to its commonsense
          understanding of English terms that appear in the PDDL.
        </p> -->
        <h3>Using Large Language Models to Solve College Math</h3>
        <p>
          <a href="assets/llms-for-math.pdf" target="_blank">[paper]</a>
          <a href="https://github.com/idrori/mathQ" target="_blank">[code]</a>
        </p>
        <p><u>Published in PNAS.</u></p>
        <!-- <p>
          <u>Paper Abstract</u>: We demonstrate that a neural network pretrained
          on text and fine-tuned on code solves mathematics course problems,
          explains solutions, and generates questions at a human level. We
          automatically synthesize programs using few-shot learning and OpenAI's
          Codex transformer and execute them to solve course problems at 81%
          automatic accuracy. We curate a dataset of questions from
          Massachusetts Institute of Technology (MIT)'s largest mathematics
          courses (Single Variable and Multivariable Calculus, Differential
          Equations, Introduction to Probability and Statistics, Linear Algebra,
          and Mathematics for Computer Science) and Columbia University's
          Computational Linear Algebra. We solve questions from a MATH dataset
          (on Prealgebra, Algebra, Counting and Probability, Intermediate
          Algebra, Number Theory, and Precalculus), the latest benchmark of
          advanced mathematics problems designed to assess mathematical
          reasoning. We randomly sample questions and generate solutions with
          multiple modalities, including numbers, equations, and plots. The
          latest GPT-3 language model pretrained on text automatically solves
          only 18.8% of these university questions using zero-shot learning and
          30.8% using few-shot learning and the most recent chain of thought
          prompting. In contrast, program synthesis with few-shot learning using
          Codex fine-tuned on code generates programs that automatically solve
          81% of these questions. Our approach improves the previous
          state-of-the-art automatic solution accuracy on the benchmark topics
          from 8.8 to 81.1%. We perform a survey to evaluate the quality and
          difficulty of generated questions. This work automatically solves
          university-level mathematics course questions at a human level and
          explains and generates university-level mathematics course questions
          at scale, a milestone for higher education.
        </p> -->
      </section>
      <!-- <hr /> -->

      <section id="projects">
        <h1>Projects</h1>
        <h3>
          Exploring the Causes & Effects of Quantization-induced Degradation in
          LLMs
        </h3>
        <p>
          <a href="assets/deep_learning_blog/index.html" target="_blank"
            >[blog]</a
          >
          <a
            href="https://github.com/reeceshuttle/deep-learning-proj"
            target="_blank"
            >[code]</a
          >
        </p>
        <p>
          We extend on recent work(Scaling Laws for Precision) showing that
          quantized models degrade in performance more significantly past a
          certain amount of training steps. We confirm their findings on larger
          models (OLMo-1B & 7B) and on downstream tasks. Further, we find that
          there are no abnormally large or growing activation statistics across
          training steps, suggesting that large activations are not the cause of
          this degradation. Lastly, we trace the quantized error for each weight
          matrix of the model and find that across training steps, the attention
          projection (\(W_{QKV}\)) weights consistently has growing quantization
          error, while other modules have roughly similar quantization error
          across steps. This suggests that Quantization-induced degradation may
          be due to growing quantization error in the queries, keys, and values
          across training steps.
        </p>

        <h3>Exploring Activation-aware Quantization for LLMs</h3>
        <p>
          <a href="assets/6_5940_Final_Project_Report.pdf" target="_blank"
            >[paper]</a
          >
          <a href="https://github.com/reeceshuttle/tinyml-proj" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Studied mixed precision and activation aware quantization in order to
          extend lossless quantization of weights and activations past 8-bit.
          Our investigation finds limitations extending quantization past 6
          bits, suggesting that achieving lossless 4-bit quantization, which can
          be leveraged for practical benefits, will be challenging to achieve.
        </p>

        <h3>Analyzing Inference Optimizations for Transformers</h3>
        <p><a href="assets/6.5930_Project.pdf" target="_blank">[paper]</a></p>
        <p>
          Studied inference optimizations, like KV-Caching and Grouped Query
          Attention, in the attention module of transformers, including in their
          impact on inference speed and energy usage.
        </p>
        <h3>Characterizing Sparsity in Transformers</h3>
        <p>
          <a href="assets/9.58-Final-Project-Report.pdf" target="_blank"
            >[paper]</a
          >
          <a href="https://github.com/reeceshuttle/958" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Theoretical and biological reasons suggest that sparsity may be
          important for deep neural network to perform well. Here, we examine
          the attention blocks of large transformer models to identify sparse
          features in their weights and/or activations. One interesting finding
          is that the weight matricies used in attention have
          <i>very low stable rank</i>, especially the matrix product
          \(W_qW_k^T\).
        </p>
        <h3>Bias in BERT models</h3>
        <p>
          <a href="assets/6.3950-Final-Project-Report.pdf" target="_blank"
            >[paper]</a
          >
          <a href="https://github.com/reeceshuttle/63950" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Historical training data used for training machine learning models can
          be biased, and machine learning models are susceptible to inheriting
          this bias. In this work, we use the masked token prediction
          capabilities of BERT models to show that they contain gender and
          racial bias. We create a dataset and use a novel loss function in
          order to reduce bias via finetuning. Preliminary analysis shows that
          this finetuning is successful at reducing bias, but needs to be
          examined further.
        </p>

        <h3>Gabor filter-constrained CNNs</h3>
        <p>
          <a href="assets/960-final-project.pdf" target="_blank">[paper]</a>
          <a
            href="https://github.com/samacqua/gabor-constrained-nns"
            target="_blank"
            >[code]</a
          >
        </p>

        <p>
          Humans have more general and robust object recognition capabilities in
          comparison to machines. In this work, we investigated whether
          constraining convolutional neural networks to be more human-like, via
          the use of Gabor filters, improves their performance and their
          robustness against adversarial attacks.
        </p>
        <h3>MIT Pokerbots</h3>
        <p>
          <a href="https://github.com/reeceshuttle/poker-bot" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          I competed in the 2023 MIT Pokerbots competition and placed in the top
          10%, resulting in a cash prize. The variant played in this competition
          was River of Blood Hold'em.
        </p>
        <h3>PyTorch, but in NumPy</h3>
        <p>
          <a href="https://github.com/reeceshuttle/numpytorch" target="_blank"
            >[code]</a
          >
        </p>
        <p>
          Implemented basic PyTorch functionality from scratch using only NumPy
          arrays. Neural networks converge and perform well on non-trivial
          problems.
        </p>
        <h3>Breadboard Computer</h3>
        <p>
          <a href="https://youtu.be/5UwkEk2dsxQ" target="_blank"
            >[Multiplication Execution Video]</a
          >
          <!-- <a href="assets/micro_instructions.png" target="_blank"
            >[Micro Instructions]</a
          > -->
          <a href="assets/multiplication_program.png" target="_blank"
            >[Multiplication Program]</a
          >
        </p>
        <p>
          Created functioning 8-bit computer by wiring logic chips on
          breadboards by hand. Created micro instructions directly in binary and
          implemented hardware primitives built out of micro instructions. Used
          16 bytes of memory to write multiplication program out of hardware
          primitives.
        </p>
      </section>
    </div>

    <footer>
      <p>Updated 07/19/2025.</p>
    </footer>
    <!-- Add your scripts or other body elements here -->
    <script src="js/index.js"></script>
    <script
      type="text/javascript"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    ></script>
  </body>
</html>
